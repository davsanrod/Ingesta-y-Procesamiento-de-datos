package ejercicios.RDD

import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.{col, explode}

object bicimad_estacines {

  def main(args: Array[String]): Unit = {

    //Con los siguientes c√≥digos Logger evitamos que salgan por pantalla
    //varias lineas de errores.
    val logger = Logger.getLogger("org.apache.spark")
    logger.setLevel(Level.ERROR)
    val logger2 = Logger.getLogger("akka")
    logger2.setLevel(Level.ERROR)
    val logger3 = Logger.getLogger("akka")
    logger3.setLevel(Level.ERROR)

    //Crear SparkSession
    val spark = SparkSession
      .builder()
      .appName("procesamiento_estaciones")
      .master("local")
      .getOrCreate()

    val sc = spark.sparkContext

    //Lectura
    val datosBiciMad = spark.read.format("json")
      .option("inferSchema", "True")
      .option("mode", "failFast")
      .load(path = "/user/dsanchez/bicicletas/FlumeData.1635589217632")

    val proc_bicimad = datosBiciMad.select(col("_id"), explode(col("stations")))

    val bicimad = proc_bicimad.select(col("_id"), col("col.*"))

    bicimad.write
      .option("header", "false")
      .mode("overwrite")
      .csv("/user/dsanchez/bicicletas/proc_estaciones")
  }


}
